services:
  spoke:
    build: ./
    container_name: spoke
    restart: always
    volumes:
      - configs:/app/wg-configs
      - ollama:/root/.ollama
    cap_add:
      - NET_ADMIN
    environment:
      - HUB_ENDPOINT=${HUB_ENDPOINT}
      - HUB_PUB_KEY=${HUB_PUB_KEY}
      - DEFAULT_PEER_PRIV_KEY=${DEFAULT_PEER_PRIV_KEY}
      - ENABLED_FOR_REQUESTS=${ENABLED_FOR_REQUESTS}
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=15m
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_DEBUG=1
      - CHUNKER_WORKERS=1
      - CHUNKER_MAX_CONCURRENT_THREADS_PER_WORKER=4
    healthcheck:
      test: "ollama --version && ollama ps || exit 1"
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    # Remove the Deploy item for CPU inference
    deploy:
          resources:
            reservations:
              devices:
              - driver: nvidia
                capabilities: ["gpu"]
                count: all
volumes:
  configs:
  ollama:
