services:
  spoke:
    build: ./
    pull_policy: always
    container_name: spoke
    restart: always
    volumes:
      - configs:/app/wg-configs
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - 11434:11434
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    cap_add:
      - NET_ADMIN
    environment:
      - HUB_ENDPOINT=${HUB_ENDPOINT}
      - HUB_PUB_KEY=${HUB_PUB_KEY}
      - DEFAULT_PEER_PRIV_KEY=${DEFAULT_PEER_PRIV_KEY}
      - ENABLED_FOR_REQUESTS=${ENABLED_FOR_REQUESTS}
      - CHUNKER_WORKERS=1
      - CHUNKER_MAX_CONCURRENT_THREADS_PER_WORKER=4
      - UUID_SERVER_NAME=${UUID_SERVER_NAME}
    # Remove the Deploy item for CPU inference
    deploy:
          resources:
            reservations:
              devices:
              - driver: nvidia
                capabilities: ["gpu"]
                count: all
volumes:
  configs:
  ollama:
